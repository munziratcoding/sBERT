{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Define callback for recording training times"
      ],
      "metadata": {
        "id": "3kPJsVzn-bdf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from timeit import default_timer as timer\n",
        "import keras\n",
        "class TimingCallback(keras.callbacks.Callback):\n",
        "    def __init__(self, logs={}):\n",
        "        self.logs=[]\n",
        "    def on_epoch_begin(self, epoch, logs={}):\n",
        "        self.starttime = timer()\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        self.logs.append(timer()-self.starttime)\n",
        "tcb = TimingCallback()"
      ],
      "metadata": {
        "id": "xgxbrVww-fy2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s0HV5GTB44RN"
      },
      "source": [
        "## sBERT Model Definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G4n_rh8544RO"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from util import load_WOS,  load_glove_embeddings,  create_embeddings_matrix\n",
        "#Implement a Transformer block as a layer\n",
        "class TransformerBlock(layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.ff_dim = ff_dim\n",
        "        self.rate = rate\n",
        "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.ffn = keras.Sequential(\n",
        "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = layers.Dropout(rate)\n",
        "        self.dropout2 = layers.Dropout(rate)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config().copy()\n",
        "        config.update({\n",
        "            'embed_dim': self.embed_dim,\n",
        "            'num_heads': self.num_heads,\n",
        "            'ff_dim': self.ff_dim,\n",
        "            'rate': self.rate\n",
        "        })\n",
        "        return config\n",
        "\n",
        "    def call(self, inputs, training):\n",
        "        attn_output = self.att(inputs, inputs)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output)\n",
        "#Implement embedding layer\n",
        "#Two seperate embedding layers, one for tokens, one for token index (positions).\n",
        "class TokenAndPositionEmbedding(layers.Layer):\n",
        "    def __init__(self, maxlen, vocab_size, embed_dim, word_index):\n",
        "        self.maxlen = maxlen\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "\n",
        "        #Toekem embedding loaded from glove\n",
        "\n",
        "        embeddings_index = load_glove_embeddings(embed_dim)\n",
        "        embeddings_matrix, found = create_embeddings_matrix(embeddings_index, word_index, embed_dim)\n",
        "\n",
        "        super(TokenAndPositionEmbedding, self).__init__()\n",
        "\n",
        "\n",
        "\n",
        "        #define position embedding\n",
        "        def getPositionEncoding(seq_len, d, n=10000):\n",
        "          P = np.zeros((seq_len, d))\n",
        "          for k in range(seq_len):\n",
        "              for i in np.arange(int(d/2)):\n",
        "                  denominator = np.power(n, 2*i/d)\n",
        "                  P[k, 2*i] = np.sin(k/denominator)\n",
        "                  P[k, 2*i+1] = np.cos(k/denominator)\n",
        "          return P\n",
        "\n",
        "        self.token_emb = layers.Embedding(input_dim = embeddings_matrix.shape[0], output_dim=embed_dim, weights = [embeddings_matrix], trainable = True)\n",
        "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim, weights = [getPositionEncoding(maxlen, embed_dim)], trainable=True)\n",
        "\n",
        "    def call(self, x):\n",
        "        maxlen = tf.shape(x)[-1]\n",
        "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
        "        positions = self.pos_emb(positions)\n",
        "        x = self.token_emb(x)\n",
        "        return x + positions\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config().copy()\n",
        "        config.update({\n",
        "            'maxlen': self.maxlen,\n",
        "            'vocab_size': self.vocab_size,\n",
        "            'embed_dim': self.embed_dim\n",
        "        })\n",
        "        return config\n",
        "\n",
        "\n",
        "def create_model(emb_dim, n_blocks, n_heads, ff_dim, maxlen, vocab_size, nClasses, word_index):\n",
        "  #print('Model : emb_dim = ', emb_dim, ', n_heads = ', n_heads, ', ff_dim = ', ff_dim)\n",
        "  inputs = layers.Input(shape=(maxlen,))\n",
        "  embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, emb_dim, word_index)\n",
        "  x = embedding_layer(inputs)\n",
        "  for i in range(n_blocks):\n",
        "    transformer_block = TransformerBlock(emb_dim, n_heads, ff_dim)\n",
        "    x = transformer_block(x)\n",
        "  #x = transformer_block(x)\n",
        "  x = layers.GlobalAveragePooling1D()(x)\n",
        "  x = layers.Dropout(0.2)(x)\n",
        "  x = layers.Dense(256, activation=\"relu\")(x)\n",
        "  x = layers.Dropout(0.2)(x)\n",
        "  outputs = layers.Dense(nClasses, activation=\"softmax\")(x)\n",
        "\n",
        "  model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "  model.compile(\"adam\", \"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Function for plotting training graphs"
      ],
      "metadata": {
        "id": "QhEwx3lPpZYE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_history(history):\n",
        "    # Extract training and validation metrics from the history object\n",
        "    training_loss = history.history['loss']\n",
        "    validation_loss = history.history['val_loss']\n",
        "    training_accuracy = history.history['accuracy']\n",
        "    validation_accuracy = history.history['val_accuracy']\n",
        "\n",
        "    epochs = range(1, len(training_loss) + 1)\n",
        "\n",
        "    # Create a pretty plot\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    # Plot training and validation loss\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(epochs, training_loss, 'bo-', label='Training Loss')\n",
        "    plt.plot(epochs, validation_loss, 'r*-', label='Validation Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    # Plot training and validation accuracy\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(epochs, training_accuracy, 'bo-', label='Training Accuracy')\n",
        "    plt.plot(epochs, validation_accuracy, 'r*-', label='Validation Accuracy')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.title('Training and Validation Accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "6LTWwNTMjmBp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hvr6kcdjdAwI"
      },
      "source": [
        "## Experiments on WOS Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cOA8tZxkyEuA"
      },
      "outputs": [],
      "source": [
        "from util import load_WOS, load_slc\n",
        "from keras.models import load_model\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import keras\n",
        "\n",
        "vocab_size = 20000 #20000  # Only consider the top 20k words\n",
        "maxlen = 250  # Only consider the first 250 words of each abstract\n",
        "emb_dim = 100\n",
        "n_blocks = 1\n",
        "n_heads = 12\n",
        "ff_dim = 256\n",
        "batchsize = 16\n",
        "num_epochs = 100\n",
        "save_dir = 'saved_models6/'\n",
        "test = False\n",
        "\n",
        "model_name = 'sBERT_' + str(n_blocks) + '_' + str(emb_dim) + '_' +  str(n_heads) + '_' +  str(ff_dim) + '_' + str(vocab_size)\n",
        "for dsname in  [\"WOS5736\",  \"WOS11967\", \"WOS46985\"]:#\n",
        "  start = \"\\033[1m\"\n",
        "  end = \"\\033[0;0m\"\n",
        "  print('\\n\\n\\n' + start + dsname + end)\n",
        "  if 'WOS' in dsname:\n",
        "    x_train, Y_train, x_test, Y_test,  word_index, nClasses  = ã…¤load_WOS(dsname, vocab_size, maxlen)\n",
        "  else:\n",
        "    x_train, Y_train, x_test, Y_test,  word_index, nClasses  =   load_slc(dsname, maxlen, vocab_size)\n",
        "  def get_model_name(model_name, dsname):\n",
        "      return model_name + '_' + dsname + '.h5'\n",
        "  model = create_model(emb_dim, n_blocks, n_heads, ff_dim, maxlen, vocab_size, nClasses, word_index)\n",
        "  if(test == True and os.path.exists(save_dir + get_model_name(model_name, dsname))):\n",
        "    model.load_weights(save_dir + get_model_name(model_name, dsname))\n",
        "    #print('Evaluating!')\n",
        "    results = model.evaluate(x=x_test, y = Y_test)\n",
        "    results = dict(zip(model.metrics_names, results))\n",
        "    print(results)\n",
        "  else:\n",
        "    es = keras.callbacks.EarlyStopping(monitor='val_accuracy', mode='max', min_delta=0.001, patience=8)\n",
        "    checkpoint = keras.callbacks.ModelCheckpoint(save_dir + get_model_name(model_name, dsname), monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
        "    tcb = TimingCallback()\n",
        "    callbacks_list = [tcb, checkpoint, es]\n",
        "    print(\"Training..\")\n",
        "    history = model.fit(x_train, Y_train,\n",
        "          epochs=10,\n",
        "          batch_size = batchsize,\n",
        "          callbacks=callbacks_list,\n",
        "          validation_data=(x_test, Y_test),\n",
        "          verbose = 1)\n",
        "    plot_history(history)\n",
        "    history.history[\"time\"]=tcb.logs\n",
        "    import pickle\n",
        "    with open(save_dir + model_name + '_' + dsname + '_trainHistoryDict', 'wb') as file_pi:\n",
        "      pickle.dump(history.history, file_pi)\n",
        "    model.load_weights(save_dir + get_model_name(model_name, dsname))\n",
        "    #print('!')\n",
        "    results = model.evaluate(x=x_test, y = Y_test)\n",
        "    results = dict(zip(model.metrics_names, results))\n",
        "    print(results)\n"
      ]
    }
  ]
}